{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10bbfac4-38dd-4247-bc38-caed563d1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mmm_fair import MMM_Fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acfc4a5-6f2b-4c8c-906d-2c796b8147b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BACK UP FOR MMM_APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15d02c76-176c-4dd5-9ebd-a36eaa5c98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from ucimlrepo import fetch_ucirepo\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9eb3739b-c46d-4893-9685-7757077c968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f=pd.read_csv(\"credit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f859493-2996-42f9-8f5f-6221e655d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isinstance(f,pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d301151c-ce31-43af-a2d6-1046631c5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f['class'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04e9e343-f602-4eb4-80ff-39e730f31b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_raw_data = fetch_ucirepo(id=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56179882-3a87-41ba-866d-45fbd75423d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_raw_data.data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "078cf9db-27a9-4e7c-bb47-906caf5d5249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_raw_data.data.targets['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1b98554-3206-4cb9-afb5-bb96d4e4810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f.drop(columns=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10fc73b0-2e8d-495b-92e0-eaeb0e5871e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643cf96-a2fc-47a5-a1b3-5c746654f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# from flask import Flask, render_template, request, session, jsonify\n",
    "# from flask_session import Session\n",
    "# import argparse\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import plotly.graph_objects as go\n",
    "# from flask import send_from_directory\n",
    "# # Import required modules\n",
    "# from mmm_fair.train_and_deploy import (\n",
    "#     generate_reports, \n",
    "#     parse_base_learner,\n",
    "#     parse_numeric_input,\n",
    "#     build_sensitives,\n",
    "#     train\n",
    "# )\n",
    "# from mmm_fair.deploy_utils import convert_to_onnx\n",
    "# from mmm_fair.viz_trade_offs import plot2d, plot3d\n",
    "# from mmm_fair.mmm_fair import MMM_Fair\n",
    "# from mmm_fair.mmm_fair_gb import MMM_Fair_GradientBoostedClassifier\n",
    "# from mmm_fair.data_process import data_uci  # Importing data processing module\n",
    "# from mmm_fair.mchat.langchain_utils import (get_langchain_agent, \n",
    "#                                     summarize_html_report, \n",
    "#                                     get_available_llm_providers)\n",
    "# from mmm_fair.mchat.openai_utils import get_openAI_llm\n",
    "# from mmm_fair.mchat.groq_utils import get_groq_llm\n",
    "\n",
    "\n",
    "# from mmm_fair.viz_trade_offs import plot_spider\n",
    "# from mmm_fair.dataset_visualization import generate_nested_pie_chart\n",
    "\n",
    "# import uuid\n",
    "# import plotly.io as pio\n",
    "# llm=None\n",
    "\n",
    "# BASE_DIR = os.path.dirname(__file__)\n",
    "# PLOT_DIR = os.path.join(BASE_DIR, \"static\")\n",
    "# os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# app = Flask(__name__,\n",
    "#             template_folder=os.path.join(BASE_DIR, \"templates\"),\n",
    "#             static_folder=PLOT_DIR)\n",
    "# app.secret_key = \"SOME_SECRET\"\n",
    "# app.config[\"SESSION_TYPE\"] = \"filesystem\"\n",
    "# Session(app)\n",
    "\n",
    "\n",
    "# THETA_DIV = \"\"\" \n",
    "# <div id=\"theta-selection\">\n",
    "#     <h3>Select Theta for Model Update</h3>\n",
    "#     <input type=\"number\" id=\"theta-input\" min=\"0\" placeholder=\"Enter Theta index\">\n",
    "#     <button id=\"theta-btn\">Update Model</button>\n",
    "\n",
    "#     <h3>Save Model</h3>\n",
    "#     <input type=\"text\" id=\"save-path\" placeholder=\"Enter directory path\">\n",
    "#     <button id=\"save-btn\">Save Onnx</button>\n",
    "# </div>\n",
    "# \"\"\"\n",
    "\n",
    "# # Define chat arguments\n",
    "# CHAT_ARGS = [\n",
    "#     \"classifier\",\n",
    "#     \"dataset\",\n",
    "#     \"prots\",\n",
    "#     \"nprotgs\",\n",
    "#     \"target\",\n",
    "#     \"pos_Class\",\n",
    "#     \"data_visualization\",\n",
    "#     \"constraint\",\n",
    "#     \"n_learners\",\n",
    "#     # \"save_as\",\n",
    "#     # \"moo_vis\"\n",
    "# ]\n",
    "\n",
    "# default_args = {\n",
    "#     \"classifier\": \"MMM_Fair_GBT\",\n",
    "#     \"dataset\": \"Adult\",\n",
    "#     \"target\": \"income\",\n",
    "#     \"pos_Class\": \">50K\",\n",
    "#     \"n_learners\": 100,\n",
    "#     \"prots\": [\"race\", \"sex\"],\n",
    "#     \"nprotgs\": [\"White\", \"Male\"],\n",
    "#     \"constraint\": \"EO\",\n",
    "#     \"save_as\": \"Onnx\",\n",
    "#     \"save_path\": \"my_mmm_fair_model\",\n",
    "#     \"base_learner\": None,\n",
    "#     \"report_type\": \"table\",\n",
    "#     \"pareto\": False,\n",
    "#     \"test\": 0.3,\n",
    "#     \"early_stop\": False,\n",
    "#     \"moo_vis\": True,\n",
    "# }\n",
    "\n",
    "\n",
    "# # Dictionary of datasets and their recommended features\n",
    "# DATASET_RECOMMENDATIONS = {\n",
    "#     \"adult\": [\"race\", \"sex\", \"age\"],\n",
    "#     \"bank\": [\"age\", \"marital\", \"education\"],\n",
    "#     \"credit\": [\"sex\", \"age\", \"married\"],\n",
    "#     \"kdd\": [\"minority\", \"gender\"],\n",
    "# }\n",
    "\n",
    "# TARGET_SUGGESTIONS = {\n",
    "#     \"adult\": [\n",
    "#         {\"value\": \"income\", \"text\": \"income\"},\n",
    "#     ],\n",
    "#     \"bank\": [\n",
    "#         {\"value\": \"y\", \"text\": \"y (subscription)\"},\n",
    "#     ],\n",
    "#     \"credit\": [\n",
    "#         {\"value\": \"default\", \"text\": \"default\"},\n",
    "#     ],\n",
    "#     \"kdd\": [\n",
    "#         {\"value\": \"income\", \"text\": \"income\"},\n",
    "#     ],\n",
    "# }\n",
    "\n",
    "# POS_CLASS_SUGGESTIONS = {\n",
    "#     \"adult\": {\n",
    "#         \"income\": [\n",
    "#             {\"value\": \">50K\", \"text\": \">50K\"},\n",
    "#             {\"value\": \"<=50K\", \"text\": \"<=50K\"},\n",
    "#         ],\n",
    "#     },\n",
    "#     \"bank\": {\n",
    "#         \"y\": [{\"value\": \"yes\", \"text\": \"yes\"}, {\"value\": \"no\", \"text\": \"no\"}],\n",
    "#     },\n",
    "#     \"credit\": {\n",
    "#         \"default\": [\n",
    "#             {\"value\": \"1\", \"text\": \"1 (defaulted)\"},\n",
    "#             {\"value\": \"0\", \"text\": \"0 (not defaulted)\"},\n",
    "#         ],\n",
    "#     },\n",
    "#     \"kdd\": {\n",
    "#         \"income\": [\n",
    "#             {\"value\": \"1\", \"text\": \"1 (high income)\"},\n",
    "#             {\"value\": \"0\", \"text\": \"0 (low income)\"},\n",
    "#         ],\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # Add this dictionary to your app.py file right after DATASET_RECOMMENDATIONS\n",
    "# NONPROTECTED_SUGGESTIONS = {\n",
    "#     \"adult\": {\n",
    "#         \"race\": [\n",
    "#             {\"value\": \"White\", \"text\": \"White\"},\n",
    "#             {\"value\": \"Black\", \"text\": \"Black\"},\n",
    "#             {\"value\": \"Asian-Pac-Islander\", \"text\": \"Asian-Pac-Islander\"},\n",
    "#         ],\n",
    "#         \"sex\": [\n",
    "#             {\"value\": \"Male\", \"text\": \"Male\"},\n",
    "#             {\"value\": \"Female\", \"text\": \"Female\"},\n",
    "#         ],\n",
    "#         \"age\": [\n",
    "#             {\"value\": \"18_30\", \"text\": \"18-30\"},\n",
    "#             {\"value\": \"30_45\", \"text\": \"30-45\"},\n",
    "#             {\"value\": \"45_60\", \"text\": \"45-60\"},\n",
    "#         ],\n",
    "#     },\n",
    "#     \"bank\": {\n",
    "#         \"age\": [\n",
    "#             {\"value\": \"18_30\", \"text\": \"18-30\"},\n",
    "#             {\"value\": \"30_45\", \"text\": \"30-45\"},\n",
    "#         ],\n",
    "#         \"marital\": [\n",
    "#             {\"value\": \"married\", \"text\": \"Married\"},\n",
    "#             {\"value\": \"single\", \"text\": \"Single\"},\n",
    "#         ],\n",
    "#         \"education\": [\n",
    "#             {\"value\": \"primary\", \"text\": \"Primary\"},\n",
    "#             {\"value\": \"secondary\", \"text\": \"Secondary\"},\n",
    "#             {\"value\": \"tertiary\", \"text\": \"Tertiary\"},\n",
    "#         ],\n",
    "#     },\n",
    "#     \"credit\": {\n",
    "#         \"sex\": [\n",
    "#             {\"value\": \"Male\", \"text\": \"Male\"},\n",
    "#             {\"value\": \"Female\", \"text\": \"Female\"},\n",
    "#         ],\n",
    "#         \"age\": [\n",
    "#             {\"value\": \"18_30\", \"text\": \"18-30\"},\n",
    "#             {\"value\": \"30_45\", \"text\": \"30-45\"},\n",
    "#         ],\n",
    "#         \"married\": [{\"value\": \"Yes\", \"text\": \"Yes\"}, {\"value\": \"No\", \"text\": \"No\"}],\n",
    "#     },\n",
    "#     \"kdd\": {\n",
    "#         \"minority\": [{\"value\": \"No\", \"text\": \"No\"}, {\"value\": \"Yes\", \"text\": \"Yes\"}],\n",
    "#         \"gender\": [{\"value\": \"M\", \"text\": \"Male\"}, {\"value\": \"F\", \"text\": \"Female\"}],\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # Dictionary to store available features for each dataset\n",
    "# DATASET_FEATURES = {}\n",
    "\n",
    "# # DEFAULT_BOT_MESSAGES = [\n",
    "# #     {\n",
    "# #         \"sender\": \"bot\",\n",
    "# #         \"text\": \"Hello! Welcome to MMM-Fair Chat.\\n\\n\"\n",
    "# #         \"I can help you train either MMM_Fair (AdaBoost style) or MMM_Fair_GBT (Gradient Boosting style).\\n\"\n",
    "# #         \"Please select an option below:\",\n",
    "# #         \"options\": [\n",
    "# #             {\"value\": \"MMM_Fair\", \"text\": \"MMM_Fair (AdaBoost)\"},\n",
    "# #             {\"value\": \"MMM_Fair_GBT\", \"text\": \"MMM_Fair_GBT (Gradient Boosting)\"},\n",
    "# #             {\"value\": \"default\", \"text\": \"Run with default parameters\"},\n",
    "# #         ],\n",
    "# #     }\n",
    "# # ]\n",
    "# DEFAULT_BOT_MESSAGES = [\n",
    "#     {\n",
    "#         \"sender\": \"bot\",\n",
    "#         \"text\": \"ðŸ‘‹ Hello! Welcome to MMM-Fair Chat.\\n\\n\"\n",
    "#                 \"Let's get started by selecting a dataset to work with.\\n\"\n",
    "#                 \"You can choose from well-known public datasets or upload your own CSV file.\",\n",
    "#         \"options\": [\n",
    "#             {\"value\": \"Adult\", \"text\": \"ðŸ§‘ Adult (UCI)\"},\n",
    "#             {\"value\": \"Bank\", \"text\": \"ðŸ¦ Bank Marketing (UCI)\"},\n",
    "#             {\"value\": \"Credit\", \"text\": \"ðŸ’³ Credit Default (UCI)\"},\n",
    "#             {\"value\": \"upload_data\", \"text\": \"ðŸ“ Upload your own Data (currently supported files: '.csv'\"},\n",
    "#         ],\n",
    "#     }\n",
    "# ]\n",
    "# @app.route(\"/\")\n",
    "# def index():\n",
    "#     session[\"llm_enabled\"] = False\n",
    "#     session[\"api_enabled\"] = False\n",
    "#     return render_template(\"index.html\")\n",
    "\n",
    "# @app.route(\"/provide_api_key\", methods=[\"POST\"])\n",
    "# def provide_api_key():\n",
    "#     try:\n",
    "#         env_key_map = {\n",
    "#                 \"openai\": \"OPENAI_API_KEY\",\n",
    "#                 \"chatgpt\": \"OPENAI_API_KEY\",\n",
    "#                 \"groq\": \"GROQ_API_KEY\",\n",
    "#                 \"groqai\": \"GROQ_API_KEY\",\n",
    "#                 \"together\": \"TOGETHER_API_KEY\"\n",
    "#             }\n",
    "#         data = request.json\n",
    "#         api_key = data.get(\"api_key\", \"\").strip()\n",
    "#         model = data.get(\"model\", \"\").strip()\n",
    "#         if not api_key:\n",
    "#             return jsonify({\"success\": False, \"error\": \"No API key provided.\"})\n",
    "\n",
    "#         if not model:\n",
    "#             return jsonify({\"success\": False, \"error\": \"No LLM provider specified.\"})\n",
    "        \n",
    "#         env_var = env_key_map.get(model.lower())\n",
    "#         if not env_var:\n",
    "#             return jsonify({\"success\": False, \"error\": f\"Unknown provider '{model}'.\"})\n",
    "            \n",
    "#         os.environ[env_var] = api_key\n",
    "#         session[\"api_enabled\"] = True\n",
    "\n",
    "#         return jsonify({\"success\": True, \"message\": \"âœ… API key accepted. LLM features are now enabled.\"})\n",
    "\n",
    "#     except ImportError:\n",
    "#         return jsonify({\n",
    "#             \"success\": False,\n",
    "#             \"error\": \"LangChain/OpenAI module not installed. Install with: pip install mmm-fair[llm-gpt]\"\n",
    "#         })\n",
    "#     except Exception as e:\n",
    "#         return jsonify({\n",
    "#             \"success\": False,\n",
    "#             \"error\": f\"Unexpected error occurred: {str(e)}\"\n",
    "#         })\n",
    "    \n",
    "\n",
    "# @app.route(\"/start_chat\", methods=[\"POST\"])\n",
    "# def start_chat():\n",
    "#     session.clear()\n",
    "#     #session[\"chat_history\"] = list(DEFAULT_BOT_MESSAGES)\n",
    "#     session[\"user_args\"] = {}\n",
    "#     session[\"plot_all_url\"]=''\n",
    "#     if session.get(\"llm_enabled\"):      \n",
    "#         template=[(\"system\",\"You are an assistant inside the MMM-Fair Chat.\"),\n",
    "#                   (\"User\", \"Greet the user politely and explain that this tool can train fairness-aware models.Use the following context: {context}\")]\n",
    "#         llm = get_openAI_llm()\n",
    "#         llm_agent=get_langchain_agent(llm,template)\n",
    "#         message = langchain_agent.invoke({\"context\": DEFAULT_BOT_MESSAGES[0][\"text\"]})\n",
    "#         session[\"chat_history\"] = [{\"sender\": \"bot\", \"text\": message}]\n",
    "#     else:\n",
    "#         session[\"chat_history\"] = list(DEFAULT_BOT_MESSAGES)\n",
    "#     return jsonify({\"ok\": True})\n",
    "\n",
    "\n",
    "# def prompt_for_llm_provider(chat_history):\n",
    "#     available = get_available_llm_providers()\n",
    "#     session[\"valid_providers\"] = available\n",
    "#     if not available:\n",
    "#         chat_history.append({\n",
    "#             \"sender\": \"bot\",\n",
    "#             \"text\": \"âš ï¸ No LLM providers are available. Please install with:\\n\"\n",
    "#                     \"`pip install mmm-fair[llm-gpt]`, `[llm-groq]`, or `[llm-together]`\"\n",
    "#         })\n",
    "#     else:\n",
    "#         session[\"llm_enabled\"]=True\n",
    "\n",
    "#         available_options = [{\"value\": opt, \"text\": opt} for opt in list(available.keys())]\n",
    "        \n",
    "#         chat_history.append({\n",
    "#             \"sender\": \"bot\",\n",
    "#             \"text\": f\"ðŸ§  Please select an available provider for explanation:\",\n",
    "#             \"options\": available_options\n",
    "#         })\n",
    "#     return chat_history\n",
    "\n",
    "# def handle_llm_provider_selection(user_msg, chat_history):\n",
    "#     \"\"\"Handle the LLM provider selection and set up for API key request.\"\"\"\n",
    "#     user_selection = user_msg.lower()\n",
    "#     valid_providers = session.get(\"valid_providers\")\n",
    "    \n",
    "#     if user_selection in valid_providers:\n",
    "#         # Set the provider in the session\n",
    "#         session[\"llm_provider\"] = user_selection\n",
    "        \n",
    "#         # Set a flag to indicate we need an API key\n",
    "#         session[\"needs_api_key\"] = True\n",
    "        \n",
    "#         # Add a message to the chat history\n",
    "#         chat_history.append({\n",
    "#             \"sender\": \"bot\",\n",
    "#             \"text\": f\"âœ… Provider '{user_selection}' selected. Setting up for API key input...\"\n",
    "#         })\n",
    "        \n",
    "#         return chat_history, True  # Return True as second value to indicate API key needed\n",
    "#     else:\n",
    "#         chat_history.append({\n",
    "#             \"sender\": \"bot\",\n",
    "#             \"text\": f\"âš ï¸ Unknown provider '{user_selection}'. Please choose one of: {', '.join(list(valid_providers.keys()))}\"\n",
    "#         })\n",
    "        \n",
    "#         return chat_history, False  # Return False to indicate no API key needed yet\n",
    "\n",
    "# @app.route(\"/ask_chat\", methods=[\"POST\"])\n",
    "# def ask_chat():\n",
    "#     chat_history = session.get(\"chat_history\", [])\n",
    "#     user_args = session.get(\"user_args\", {})\n",
    "#     user_msg = request.json.get(\"message\", \"\").strip()\n",
    "#     button_value = request.json.get(\"button_value\", \"\")\n",
    "#     selected_features = request.json.get(\n",
    "#         \"selected_features\", []\n",
    "#     )  # Get selected features if provided\n",
    "\n",
    "#     # Add this to handle non-protected group selection\n",
    "#     nprotgs_value = request.json.get(\"nprotgs_value\", \"\")\n",
    "\n",
    "#     print(f\"DEBUG: Received button_value: {button_value}\")\n",
    "\n",
    "#     # If a button was clicked, use its value instead of the message\n",
    "#     if button_value:\n",
    "#         user_msg = button_value\n",
    "\n",
    "#     if session.get(\"last_action\") == \"plotted\" and user_msg.lower() in [\"yes\", \"explain\", \"please explain\", \"what do these mean?\", \"openai\", \"chatgpt\", \"groqai\", \"groq\", \"togetherai\", \"together\"]:\n",
    "#         if not session.get(\"llm_enabled\"):\n",
    "#             session[\"llm_provider\"] = \"in-progress\"\n",
    "#             chat_history = prompt_for_llm_provider(chat_history)\n",
    "#             session[\"chat_history\"] = chat_history\n",
    "            \n",
    "#             return jsonify({\"chat_history\": chat_history[-1:]})\n",
    "            \n",
    "#         elif session.get(\"llm_provider\") and len(session.get(\"valid_providers\", {})) > 0:\n",
    "#             if session.get(\"llm_provider\") == \"in-progress\":\n",
    "#                 # Process the LLM provider selection\n",
    "#                 updated_history, needs_api_key = handle_llm_provider_selection(user_msg, chat_history)\n",
    "                \n",
    "#                 # Update the chat history\n",
    "#                 chat_history = updated_history\n",
    "#                 session[\"chat_history\"] = chat_history\n",
    "                \n",
    "#                 # If we need an API key, indicate that to the frontend\n",
    "#                 if needs_api_key:\n",
    "#                     return jsonify({\n",
    "#                         \"chat_history\": chat_history[-1:],\n",
    "#                         \"require_api_key\": True,\n",
    "#                         \"provider\": session.get(\"llm_provider\")\n",
    "#                     })\n",
    "                \n",
    "#                 return jsonify({\"chat_history\": chat_history[-1:]})\n",
    "                \n",
    "#             else:\n",
    "#                 # Provider already selected, check if we need API key\n",
    "#                 if not session.get(\"api_enabled\"):\n",
    "#                     return jsonify({\n",
    "#                         \"require_api_key\": True, \n",
    "#                         \"provider\": session.get(\"llm_provider\"),\n",
    "#                         \"chat_history\": [{\n",
    "#                             \"sender\": \"bot\",\n",
    "#                             \"text\": f\"Please provide your {session.get('llm_provider')} API key to continue.\"\n",
    "#                         }]\n",
    "#                     })\n",
    "                \n",
    "#                 # If we have API key, generate explanation\n",
    "#                 try:\n",
    "#                     # Get relevant data for explanation\n",
    "#                     table_id = session.get(\"table_id\", 0)\n",
    "#                     all_plots = session.get(\"plots\")\n",
    "\n",
    "#                     table_plot = [item for item in all_plots if item.get(\"id\") == table_id][0]\n",
    "#                     table_plot_html = table_plot[\"srcdoc\"]\n",
    "\n",
    "#                     providers = session.get(\"valid_providers\")\n",
    "#                     selected_model = session.get(\"llm_provider\")\n",
    "#                     llm = providers[selected_model]()\n",
    "#                     template = [\n",
    "#                         (\"system\", \"You are a fairness analysis expert, specially skilled to point out trade-offs between different performance metrics and fairness metrics, and gives suggestive guidance to user where fairness needs to be improved.\"),\n",
    "#                         (\"user\", \"Please explain in brief summary the following given report focusing mainly on the most important numbers about fairness and predictive performance across protected attributes:\\n{context}\\n\\n\")\n",
    "#                     ]\n",
    "#                     llm_agent = get_langchain_agent(llm, template)\n",
    "#                     summary_response = summarize_html_report(table_plot_html, llm_agent)\n",
    "                    \n",
    "#                 except Exception as e:\n",
    "#                     if \"openai\" in str(e).lower() or \"key\" in str(e).lower() or \"quota\" in str(e).lower():\n",
    "#                         return jsonify({\n",
    "#                             \"require_api_key\": True, \n",
    "#                             \"provider\": session.get(\"llm_provider\"),\n",
    "#                             \"chat_history\": [{\n",
    "#                                 \"sender\": \"bot\",\n",
    "#                                 \"text\": f\"There was an issue with your API key. Please provide a valid {session.get('llm_provider')} API key.\"\n",
    "#                             }]\n",
    "#                         })\n",
    "                \n",
    "#                     summary_response = f\"(âš ï¸ Could not summarize the plot: {e})\"\n",
    "            \n",
    "#                 chat_history.append({\"sender\": \"bot\", \"text\": summary_response})\n",
    "#                 # Clean up session\n",
    "#                 if \"valid_providers\" in session:\n",
    "#                     del session[\"valid_providers\"]\n",
    "#                 session[\"last_action\"] = \"summarized\"\n",
    "#                 session[\"chat_history\"] = chat_history\n",
    "#                 return jsonify({\"chat_history\": chat_history[-1:]})\n",
    "        \n",
    "#         elif session.get(\"llm_provider\") == \"in-progress\" and len(session.get(\"valid_providers\", {})) == 0:\n",
    "#             chat_history.append({\n",
    "#                 \"sender\": \"bot\",\n",
    "#                 \"text\": \"You can also choose a Theta index below to update your model, and corresponding model's performance will also be updated. To start again with another Data/ Model, click on 'reset chat' ðŸ™‚\"\n",
    "#             })\n",
    "#             return jsonify({\"chat_history\": chat_history[-1:]})\n",
    "        \n",
    "#     # Handle non-protected group selection\n",
    "#     if user_msg.startswith(\"nprotg_\") and nprotgs_value:\n",
    "#         # Initialize nprotgs if not exists\n",
    "#         if \"nprotgs_temp\" not in session:\n",
    "#             session[\"nprotgs_temp\"] = []\n",
    "\n",
    "#         # Add the new value\n",
    "#         session[\"nprotgs_temp\"].append(nprotgs_value)\n",
    "\n",
    "#         # Check if we have enough values\n",
    "#         if len(session[\"nprotgs_temp\"]) >= len(user_args.get(\"prots\", [])):\n",
    "#             # We have all values, combine them\n",
    "#             user_args[\"nprotgs\"] = session[\"nprotgs_temp\"]\n",
    "#             chat_history.append(\n",
    "#                 {\n",
    "#                     \"sender\": \"bot\",\n",
    "#                     \"text\": f\"Set nprotgs = {' '.join(session['nprotgs_temp'])}\",\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#             # Clear temporary storage\n",
    "#             session.pop(\"nprotgs_temp\", None)\n",
    "\n",
    "#             # Continue to next parameter\n",
    "#             new_missing = get_missing_args(user_args)\n",
    "#             if new_missing:\n",
    "#                 next_arg = new_missing[0]\n",
    "#                 prompt, options = get_prompt_for_arg(next_arg, user_args)\n",
    "#                 chat_history.append(\n",
    "#                     {\"sender\": \"bot\", \"text\": prompt, \"options\": options}\n",
    "#                 )\n",
    "#             else:\n",
    "#                 chat_history.append(\n",
    "#                     {\n",
    "#                         \"sender\": \"bot\",\n",
    "#                         \"text\": \"All arguments captured. What would you like to do?\",\n",
    "#                         \"options\": [\n",
    "#                             {\"value\": \"run\", \"text\": \"Run Training\"},\n",
    "#                             {\"value\": \"reset\", \"text\": \"Start Over\"},\n",
    "#                         ],\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#             session[\"chat_history\"] = chat_history\n",
    "#             session[\"user_args\"] = user_args\n",
    "#             return jsonify({\"chat_history\": chat_history[-2:]})\n",
    "#         else:\n",
    "#             # Still need more values, show progress\n",
    "#             remaining = len(user_args.get(\"prots\", [])) - len(session[\"nprotgs_temp\"])\n",
    "#             protected_attrs = user_args.get(\"prots\", [])\n",
    "#             chat_history.append(\n",
    "#                 {\n",
    "#                     \"sender\": \"bot\",\n",
    "#                     \"text\": f\"Selected {nprotgs_value} for {protected_attrs[len(session['nprotgs_temp'])-1]}. \"\n",
    "#                     + f\"Please select {remaining} more value(s) for: {', '.join(protected_attrs[len(session['nprotgs_temp']):])}\",\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#             # Show options for the next protected attribute\n",
    "#             next_prot = protected_attrs[len(session[\"nprotgs_temp\"])]\n",
    "#             dataset_name = user_args.get(\"dataset\", \"\").lower()\n",
    "\n",
    "#             if (\n",
    "#                 dataset_name in NONPROTECTED_SUGGESTIONS\n",
    "#                 and next_prot in NONPROTECTED_SUGGESTIONS[dataset_name]\n",
    "#             ):\n",
    "#                 options = NONPROTECTED_SUGGESTIONS[dataset_name][next_prot]\n",
    "#                 chat_history.append(\n",
    "#                     {\n",
    "#                         \"sender\": \"bot\",\n",
    "#                         \"text\": f\"Select non-protected value for {next_prot}:\",\n",
    "#                         \"options\": [\n",
    "#                             {\n",
    "#                                 \"value\": f\"nprotg_{next_prot}_{option['value']}\",\n",
    "#                                 \"text\": option[\"text\"],\n",
    "#                             }\n",
    "#                             for option in options\n",
    "#                         ],\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#             session[\"chat_history\"] = chat_history\n",
    "#             return jsonify({\"chat_history\": chat_history[-2:]})\n",
    "\n",
    "#     # Handle feature selection submission\n",
    "#     if user_msg == \"submit_features\" and selected_features:\n",
    "#         # Process submitted features\n",
    "#         if not selected_features:\n",
    "#             chat_history.append(\n",
    "#                 {\n",
    "#                     \"sender\": \"bot\",\n",
    "#                     \"text\": \"Error: Please select at least one protected attribute.\",\n",
    "#                 }\n",
    "#             )\n",
    "#         else:\n",
    "#             user_args[\"prots\"] = selected_features\n",
    "#             chat_history.append(\n",
    "#                 {\n",
    "#                     \"sender\": \"bot\",\n",
    "#                     \"text\": f\"Set protected attributes = {', '.join(selected_features)}\",\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#             # Continue to next parameter\n",
    "#             new_missing = get_missing_args(user_args)\n",
    "#             if new_missing:\n",
    "#                 next_arg = new_missing[0]\n",
    "#                 prompt, options = get_prompt_for_arg(next_arg, user_args)\n",
    "\n",
    "#                 message = {\"sender\": \"bot\", \"text\": prompt}\n",
    "#                 if options:\n",
    "#                     message[\"options\"] = options\n",
    "\n",
    "#                 chat_history.append(message)\n",
    "#             else:\n",
    "#                 chat_history.append(\n",
    "#                     {\n",
    "#                         \"sender\": \"bot\",\n",
    "#                         \"text\": \"All arguments captured. What would you like to do?\",\n",
    "#                         \"options\": [\n",
    "#                             {\"value\": \"run\", \"text\": \"Run Training\"},\n",
    "#                             {\"value\": \"reset\", \"text\": \"Start Over\"},\n",
    "#                         ],\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#             session[\"chat_history\"] = chat_history\n",
    "#             session[\"user_args\"] = user_args\n",
    "#             return jsonify({\"chat_history\": chat_history[-2:]})\n",
    "\n",
    "#     if user_msg == \"visualize_yes\":\n",
    "#         # Store in user_args so we don't ask again\n",
    "#         user_args[\"data_visualization\"] = \"show_visualization\"\n",
    "#         session[\"user_args\"] = user_args\n",
    "#         # User wants to visualize data\n",
    "#         return jsonify({\"redirect\": \"/visualize_data\"})\n",
    "#     elif user_msg == \"visualize_no\":\n",
    "#         # Store in user_args so we don't ask again\n",
    "#         user_args[\"data_visualization\"] = \"skip_visualization\"\n",
    "#         session[\"user_args\"] = user_args\n",
    "#         # User doesn't want visualization, proceed to next step\n",
    "#         new_missing = get_missing_args(user_args)\n",
    "#         if new_missing:\n",
    "#             next_arg = new_missing[0]\n",
    "#             prompt, options = get_prompt_for_arg(next_arg, user_args)\n",
    "\n",
    "#             message = {\"sender\": \"bot\", \"text\": prompt}\n",
    "#             if options:\n",
    "#                 message[\"options\"] = options\n",
    "\n",
    "#             chat_history.append(message)\n",
    "#         else:\n",
    "#             chat_history.append(\n",
    "#                 {\n",
    "#                     \"sender\": \"bot\",\n",
    "#                     \"text\": \"All arguments captured. What would you like to do?\",\n",
    "#                     \"options\": [\n",
    "#                         {\"value\": \"run\", \"text\": \"Run Training\"},\n",
    "#                         {\"value\": \"reset\", \"text\": \"Start Over\"},\n",
    "#                     ],\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#         session[\"chat_history\"] = chat_history\n",
    "#         return jsonify({\"chat_history\": chat_history[-1:]})\n",
    "\n",
    "#     # **NEW FEATURE**: If user chooses \"default\", skip everything and run with default_args\n",
    "#     if not user_args and user_msg.lower() == \"default\":\n",
    "#         for arg in default_args:\n",
    "#             if arg not in user_args:\n",
    "#                 user_args[arg] = default_args[arg]\n",
    "#         session[\"classifier\"] = user_args[\"classifier\"]\n",
    "#         chat_history.append(\n",
    "#             {\"sender\": \"bot\", \"text\": \"Running MMM_Fair with default parameters...\"}\n",
    "#         )\n",
    "#         visualizations, html_divs = run_mmm_fair_app(default_args)\n",
    "\n",
    "#         session[\"last_action\"] = \"plotted\"\n",
    "\n",
    "#         default_resp=\"âœ… Training complete! Here in the generated pareto plots (upper box) you see various solution points (models) available, where each solution point (denoted by theta: number) shows a different trade-off point between the different training objectives. \\n\\n The performance of the suggested best model is also plotted (lower box). You can also choose a Theta index below to update your model, and corresponding model's performance will also be updated.\"\n",
    "\n",
    "#         return jsonify(\n",
    "#             {\n",
    "#                 \"chat_history\": [\n",
    "#                     {\n",
    "#                         \"sender\": \"bot\",\n",
    "#                         \"text\": f\"{default_resp}\\n\\n If you'd like me to explain what you're seeing?.\\n\",\n",
    "#                         \"options\": [\n",
    "#                             {\"value\": \"explain\", \"text\": \"Yes, please explain\"},\n",
    "#                             {\"value\": \"reset\", \"text\": \"Start Over\"},\n",
    "#                         ],\n",
    "#                     }\n",
    "#                 ],\n",
    "#                 \"plots\": visualizations,\n",
    "#                 \"html_divs\": html_divs,\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     # Add user message\n",
    "#     if user_msg:\n",
    "#         # For button clicks, show the button text instead of value\n",
    "#         display_text = user_msg\n",
    "#         if chat_history:  # Check if chat_history is not empty\n",
    "#             for option in chat_history[-1].get(\"options\", []):\n",
    "#                 if option[\"value\"] == user_msg:\n",
    "#                     display_text = option[\"text\"]\n",
    "#                     break\n",
    "\n",
    "#         chat_history.append({\"sender\": \"user\", \"text\": display_text})\n",
    "\n",
    "#     missing_args = get_missing_args(user_args)\n",
    "\n",
    "#     if not missing_args:\n",
    "#         if user_msg.lower() == \"run\":\n",
    "#             for arg in default_args:\n",
    "#                 if arg not in user_args:\n",
    "#                     user_args[arg] = default_args[arg]\n",
    "#             session[\"classifier\"] = user_args[\"classifier\"]\n",
    "#             visualizations, html_divs = run_mmm_fair_app(user_args)\n",
    "\n",
    "#             session[\"last_action\"] = \"plotted\"\n",
    "#             default_resp=\"âœ… Training complete! Here in the generated pareto plots (upper box) you see various solution points (models) available, where each solution point (denoted by theta: number) shows a different trade-off point between the different training objectives. \\n\\n The performance of the suggested best model is also plotted (lower box). You can also choose a Theta index below to update your model, and corresponding model's performance will also be updated.\"\n",
    "\n",
    "#             return jsonify(\n",
    "#                 {\n",
    "#                     \"chat_history\": [\n",
    "#                         {\n",
    "#                             \"sender\": \"bot\",\n",
    "#                             \"text\": f\"{default_resp}\\n\\n If you'd like me to explain what you're seeing?.\\n\",\n",
    "#                             \"options\": [\n",
    "#                                 {\"value\": \"explain\", \"text\": \"Yes, please explain\"},\n",
    "#                                 {\"value\": \"reset\", \"text\": \"Start Over\"},\n",
    "#                             ],\n",
    "#                         }\n",
    "#                     ],\n",
    "#                     \"plots\": visualizations,\n",
    "#                     \"html_divs\": html_divs,\n",
    "\n",
    "#                 }\n",
    "#             )\n",
    "#         else:\n",
    "#             chat_history.append(\n",
    "#                 {\n",
    "#                     \"sender\": \"bot\",\n",
    "#                     \"text\": \"We have all arguments. What would you like to do?\",\n",
    "#                     \"options\": [\n",
    "#                         {\"value\": \"run\", \"text\": \"Run Training\"},\n",
    "#                         {\"value\": \"reset\", \"text\": \"Start Over\"},\n",
    "#                     ],\n",
    "#                 }\n",
    "#             )\n",
    "#     else:\n",
    "#         current_arg = missing_args[0]\n",
    "#         print(missing_args)\n",
    "#         valid, clean_val, err_msg = validate_arg(current_arg, user_msg, user_args)\n",
    "\n",
    "#         # Process classifier selection (from buttons)\n",
    "#         if current_arg == \"classifier\":\n",
    "#             if user_msg in [\"MMM_Fair\", \"MMM_Fair_GBT\"]:\n",
    "#                 user_args[current_arg] = user_msg\n",
    "#                 valid = True\n",
    "#                 clean_val = user_msg\n",
    "\n",
    "#         # **NEW FEATURE**: Load dataset after fetching \"dataset\" argument\n",
    "#         if current_arg == \"dataset\":\n",
    "#             if valid:\n",
    "#                 # Load dataset and store for later use\n",
    "#                 try:\n",
    "#                     session[\"data\"] = data_uci(clean_val)  # Load dataset\n",
    "#                     user_args[current_arg] = clean_val\n",
    "#                     chat_history.append(\n",
    "#                         {\"sender\": \"bot\", \"text\": f\"Set {current_arg} = {clean_val}.\"}\n",
    "#                     )\n",
    "#                     chat_history.append(\n",
    "#                         {\"sender\": \"bot\", \"text\": \"Dataset loaded successfully.\"}\n",
    "#                     )\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     chat_history.append(\n",
    "#                         {\n",
    "#                             \"sender\": \"bot\",\n",
    "#                             \"text\": f\"Error loading dataset: {str(e)}\\nPlease select a valid dataset.\",\n",
    "#                             \"options\": [\n",
    "#                                 {\"value\": \"adult\", \"text\": \"Adult Dataset\"},\n",
    "#                                 {\"value\": \"bank\", \"text\": \"Bank Dataset\"},\n",
    "#                                 {\"value\": \"credit\", \"text\": \"Credit Dataset\"},\n",
    "#                                 {\"value\": \"kdd\", \"text\": \"KDD Dataset\"},\n",
    "#                             ],\n",
    "#                         }\n",
    "#                     )\n",
    "#                     session[\"chat_history\"] = chat_history\n",
    "#                     return jsonify({\"chat_history\": chat_history[-2:]})\n",
    "\n",
    "#             else:\n",
    "#                 chat_history.append(\n",
    "#                     {\n",
    "#                         \"sender\": \"bot\",\n",
    "#                         \"text\": f\"Error: {err_msg}\\nPlease select a dataset:\",\n",
    "#                         \"options\": [\n",
    "#                             {\"value\": \"adult\", \"text\": \"Adult Dataset\"},\n",
    "#                             {\"value\": \"bank\", \"text\": \"Bank Dataset\"},\n",
    "#                             {\"value\": \"credit\", \"text\": \"Credit Dataset\"},\n",
    "#                             {\"value\": \"kdd\", \"text\": \"KDD Dataset\"},\n",
    "#                         ],\n",
    "#                     }\n",
    "#                 )\n",
    "#                 session[\"chat_history\"] = chat_history\n",
    "#                 return jsonify({\"chat_history\": chat_history[-2:]})\n",
    "\n",
    "#         # Special handling for protected attributes\n",
    "#         elif current_arg == \"prots\":\n",
    "#             # Instead of validating text input, we'll create the feature selector UI\n",
    "#             prompt, options = get_prompt_for_arg(current_arg, user_args)\n",
    "\n",
    "#             message = {\"sender\": \"bot\", \"text\": prompt}\n",
    "\n",
    "#             if options:\n",
    "#                 message[\"options\"] = options\n",
    "\n",
    "#             chat_history.append(message)\n",
    "#             session[\"chat_history\"] = chat_history\n",
    "#             return jsonify({\"chat_history\": chat_history[-1:]})\n",
    "\n",
    "#         elif current_arg == \"constraint\":\n",
    "#             if not valid:\n",
    "#                 chat_history.append(\n",
    "#                     {\n",
    "#                         \"sender\": \"bot\",\n",
    "#                         \"text\": f\"Error: {err_msg}\\nPlease select a fairness constraint:\",\n",
    "#                         \"options\": [\n",
    "#                             {\"value\": \"DP\", \"text\": \"Demographic Parity (DP)\"},\n",
    "#                             {\"value\": \"EP\", \"text\": \"Equal Precision (EP)\"},\n",
    "#                             {\"value\": \"EO\", \"text\": \"Equal Opportunity (EO)\"},\n",
    "#                             {\"value\": \"TPR\", \"text\": \"True Positive Rate (TPR)\"},\n",
    "#                             {\"value\": \"FPR\", \"text\": \"False Positive Rate (FPR)\"},\n",
    "#                         ],\n",
    "#                     }\n",
    "#                 )\n",
    "#                 session[\"chat_history\"] = chat_history\n",
    "#                 return jsonify({\"chat_history\": chat_history[-2:]})\n",
    "#             else:\n",
    "#                 user_args[current_arg] = clean_val\n",
    "#                 chat_history.append(\n",
    "#                     {\"sender\": \"bot\", \"text\": f\"Set {current_arg} = {clean_val}.\"}\n",
    "#                 )\n",
    "\n",
    "#         elif current_arg == \"data_visualization\" and valid:\n",
    "#             user_args[current_arg] = clean_val\n",
    "\n",
    "#             if clean_val == \"show_visualization\":\n",
    "#                 # Redirect to the visualization endpoint\n",
    "#                 return jsonify({\"redirect\": \"/visualize_data\"})\n",
    "#             else:\n",
    "#                 # Just set the value and continue to next argument\n",
    "#                 chat_history.append(\n",
    "#                     {\"sender\": \"bot\", \"text\": \"Continuing without data visualization.\"}\n",
    "#                 )\n",
    "\n",
    "#                 # Now process as usual for the next missing argument\n",
    "#                 new_missing = get_missing_args(user_args)\n",
    "#                 if new_missing:\n",
    "#                     next_arg = new_missing[0]\n",
    "#                     prompt, options = get_prompt_for_arg(next_arg, user_args)\n",
    "\n",
    "#                     message = {\"sender\": \"bot\", \"text\": prompt}\n",
    "#                     if options:\n",
    "#                         message[\"options\"] = options\n",
    "\n",
    "#                     chat_history.append(message)\n",
    "#                 else:\n",
    "#                     chat_history.append(\n",
    "#                         {\n",
    "#                             \"sender\": \"bot\",\n",
    "#                             \"text\": \"All arguments captured. What would you like to do?\",\n",
    "#                             \"options\": [\n",
    "#                                 {\"value\": \"run\", \"text\": \"Run Training\"},\n",
    "#                                 {\"value\": \"reset\", \"text\": \"Start Over\"},\n",
    "#                             ],\n",
    "#                         }\n",
    "#                     )\n",
    "\n",
    "#                 session[\"chat_history\"] = chat_history\n",
    "#                 session[\"user_args\"] = user_args\n",
    "#                 return jsonify({\"chat_history\": chat_history[-2:]})\n",
    "\n",
    "#         else:\n",
    "#             # For other arguments, still process text input but provide buttons when possible\n",
    "#             if not valid:\n",
    "#                 chat_history.append(\n",
    "#                     {\n",
    "#                         \"sender\": \"bot\",\n",
    "#                         \"text\": f\"Error: {err_msg}\\nPlease re-enter {current_arg}.\",\n",
    "#                     }\n",
    "#                 )\n",
    "#             else:\n",
    "#                 user_args[current_arg] = clean_val\n",
    "#                 chat_history.append(\n",
    "#                     {\"sender\": \"bot\", \"text\": f\"Set {current_arg} = {clean_val}.\"}\n",
    "#                 )\n",
    "\n",
    "#         new_missing = get_missing_args(user_args)\n",
    "#         if new_missing:\n",
    "#             next_arg = new_missing[0]\n",
    "#             prompt, options = get_prompt_for_arg(next_arg, user_args)\n",
    "\n",
    "#             message = {\"sender\": \"bot\", \"text\": prompt}\n",
    "#             if options:\n",
    "#                 message[\"options\"] = options\n",
    "\n",
    "#             chat_history.append(message)\n",
    "#         else:\n",
    "#             chat_history.append(\n",
    "#                 {\n",
    "#                     \"sender\": \"bot\",\n",
    "#                     \"text\": \"All arguments captured. What would you like to do?\",\n",
    "#                     \"options\": [\n",
    "#                         {\"value\": \"run\", \"text\": \"Run Training\"},\n",
    "#                         {\"value\": \"reset\", \"text\": \"Start Over\"},\n",
    "#                     ],\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     session[\"chat_history\"] = chat_history\n",
    "#     session[\"user_args\"] = user_args\n",
    "#     return jsonify({\"chat_history\": chat_history[-2:]})\n",
    "\n",
    "\n",
    "# @app.route(\"/update_model\", methods=[\"POST\"])\n",
    "# def update_model():\n",
    "#     # Get Theta value from the request\n",
    "#     data = request.json\n",
    "#     theta_value = int(data.get(\"theta\", -1))\n",
    "\n",
    "#     # Retrieve trained classifier\n",
    "#     mmm_classifier = session.get(\"mmm_classifier\")\n",
    "#     if not mmm_classifier:\n",
    "#         return jsonify(\n",
    "#             {\"success\": False, \"error\": \"No trained model found! Run training first.\"}\n",
    "#         )\n",
    "\n",
    "#     # Validate Theta index\n",
    "#     if theta_value < 0 or theta_value >= len(mmm_classifier.ob):\n",
    "#         return jsonify(\n",
    "#             {\n",
    "#                 \"success\": False,\n",
    "#                 \"error\": f\"Invalid Theta index. Please select between 0 and {len(mmm_classifier.ob) - 1}.\",\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     # Update the model with selected Theta\n",
    "#     mmm_classifier.update_theta(theta=theta_value)\n",
    "#     session[\"mmm_classifier\"] = mmm_classifier  # Store updated model\n",
    "\n",
    "#     X_test = session.get(\"xtest\")\n",
    "#     y_test = session.get(\"ytest\")\n",
    "#     user_args = session.get(\"user_args\", {})\n",
    "#     sensitives = session.get(\"sensitives\")\n",
    "#     saIndex_test = session.get(\"saIndex_test\")\n",
    "\n",
    "#     y_pred = mmm_classifier.predict(X_test)\n",
    "\n",
    "#     report_table = generate_reports(\n",
    "#         \"html\", sensitives, mmm_classifier, saIndex_test, y_pred, y_test, html=True\n",
    "#     )\n",
    "\n",
    "#     table_id = session.get(\"table_id\", 0)\n",
    "\n",
    "#     print(f\"DEBUG: Updating report table with ID: {table_id}\")\n",
    "\n",
    "#     report_table_plot_dict = {\n",
    "#         \"updated_data\":\n",
    "#         {\n",
    "#             \"srcdoc\": report_table,\n",
    "#         },\n",
    "#         \"existing_id\": table_id\n",
    "#     }\n",
    "\n",
    "\n",
    "#     # unique_id = str(uuid.uuid4())[:4]\n",
    "#     # plot_table = \"table_.html\"\n",
    "#     # p2 = f\"table_{unique_id}.html\"\n",
    "#     # plot_table_path = os.path.join(PLOT_DIR, p2)  # plot_table)\n",
    "\n",
    "#     # with open(plot_table_path, \"w\") as f:\n",
    "#     #     f.write(report_table)\n",
    "#     #     f.flush()\n",
    "#     #     os.fsync(f.fileno())\n",
    "\n",
    "#     # session[\"plot_fair_url\"] = f\"/static/{p2}\"\n",
    "#     return jsonify(\n",
    "#         {\n",
    "#             \"success\": True,\n",
    "#             \"message\": f\"Model updated with Theta index {theta_value}.\",\n",
    "#             # \"plot_fair_url\": f\"/static/{plot_table}\",\n",
    "#             \"update_plots\": [report_table_plot_dict],\n",
    "#             \"chat_history\": [\n",
    "#                 {\n",
    "#                     \"sender\": \"bot\",\n",
    "#                     \"text\": f\"Model updated with Theta index {theta_value}. Would you like to save this model?\",\n",
    "#                     \"options\": [\n",
    "#                         {\"value\": \"save_model\", \"text\": \"Save Model\"},\n",
    "#                         {\"value\": \"reset\", \"text\": \"Start Over\"},\n",
    "#                     ],\n",
    "#                 }\n",
    "#             ],\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "\n",
    "# def get_missing_args(user_args):\n",
    "#     \"\"\"\n",
    "#     Return the list of arguments we still need, considering\n",
    "#     if user chose MMM_Fair_GBT => skip base_learner\n",
    "#     \"\"\"\n",
    "#     chosen_classifier = user_args.get(\"classifier\", \"\").lower()\n",
    "\n",
    "#     if not chosen_classifier:\n",
    "#         return [\"classifier\"]  # No classifier => we can't skip anything yet\n",
    "\n",
    "#     needed_args = []\n",
    "#     for arg in CHAT_ARGS:\n",
    "#         if arg == \"base_learner\" and chosen_classifier in [\n",
    "#             \"mmm_fair_gbt\",\n",
    "#             \"mmm-fair-gbt\",\n",
    "#         ]:\n",
    "#             continue  # Skip base_learner for GBT models\n",
    "\n",
    "#         if arg == \"data_visualization\":\n",
    "#             has_target = \"target\" in user_args and user_args[\"target\"]\n",
    "#             has_prots = \"prots\" in user_args and user_args[\"prots\"]\n",
    "\n",
    "#             if has_target and has_prots and \"data_visualization\" not in user_args:\n",
    "#                 needed_args.append(arg)\n",
    "#             continue  # Skip to next argument regardless\n",
    "\n",
    "#         if arg not in user_args:\n",
    "#             needed_args.append(arg)\n",
    "\n",
    "#     return needed_args\n",
    "\n",
    "\n",
    "# def validate_arg(arg_name, user_input, user_args):\n",
    "#     \"\"\"\n",
    "#     Simple validator that can parse or store defaults.\n",
    "#     Returns (valid, clean_value, error_message).\n",
    "#     \"\"\"\n",
    "#     print(arg_name, user_input, user_args)\n",
    "#     if arg_name == \"classifier\":\n",
    "#         val = user_input.lower()\n",
    "#         if val not in [\"mmm_fair\", \"mmm_fair_gbt\", \"mmm-fair\", \"mmm-fair-gbt\"]:\n",
    "#             return False, None, \"Classifier must be 'MMM_Fair' or 'MMM_Fair_GBT'.\"\n",
    "#         return True, \"MMM_Fair_GBT\" if \"gbt\" in val else \"MMM_Fair\", \"\"\n",
    "\n",
    "#     elif arg_name == \"n_learners\":\n",
    "#         if not user_input.isdigit():\n",
    "#             return False, None, \"n_learners must be an integer e.g. 100\"\n",
    "#         return True, int(user_input), \"\"\n",
    "\n",
    "#     elif arg_name == \"constraint\":\n",
    "#         c = user_input.upper()\n",
    "#         if c not in [\"DP\", \"EP\", \"EO\", \"TPR\", \"FPR\"]:\n",
    "#             return False, None, \"Constraint must be DP, EP, EO, TPR, or FPR.\"\n",
    "#         return True, c, \"\"\n",
    "\n",
    "#     elif arg_name == \"dataset\":\n",
    "#         if user_input.lower().endswith(\".csv\"):\n",
    "#             return True, user_input, \"\"\n",
    "#         elif user_input.lower() in [\"adult\", \"bank\", \"kdd\", \"credit\"]:\n",
    "#             return True, user_input.lower(), \"Loading Data...please wait!!!\"\n",
    "#         else:\n",
    "#             return False, user_input, \"\"  # Fallback case\n",
    "\n",
    "#     elif arg_name == \"target\":\n",
    "#         # Extract the name of the target column\n",
    "#         available_target_name = (\n",
    "#             session[\"data\"].labels[\"label\"].name\n",
    "#         )  # Get the column name\n",
    "\n",
    "#         print(\n",
    "#             f\"DEBUG: Available target name: {available_target_name}\"\n",
    "#         )  # Debugging output\n",
    "\n",
    "#         return True, user_input, \"\"\n",
    "\n",
    "#     elif arg_name == \"data_visualization\":\n",
    "#         if user_input.lower() in [\"visualize_yes\", \"yes\", \"y\"]:\n",
    "#             # Instead of storing a simple value, store a flag to redirect\n",
    "#             # This will be handled specially in ask_chat\n",
    "#             return True, \"show_visualization\", \"\"\n",
    "#         else:\n",
    "#             return True, \"skip_visualization\", \"\"\n",
    "\n",
    "#     elif arg_name == \"pos_Class\":\n",
    "#         if not user_input:\n",
    "#             return True, None, \"\"  # Default None\n",
    "#         return True, user_input, \"\"\n",
    "\n",
    "#     elif arg_name == \"prots\":\n",
    "#         available_columns = session[\"data\"].data.columns.tolist()\n",
    "#         available_columns = [v.lower() for v in available_columns]\n",
    "#         if not user_input:\n",
    "#             return False, [], \"At least one protected attribute expected\"\n",
    "#         else:\n",
    "#             k = \"\"\n",
    "#             for prot in user_input.split(\" \"):\n",
    "#                 if prot.lower() not in available_columns:\n",
    "#                     chosen_dataset = user_args.get(\"dataset\", \"\").lower()\n",
    "#                     if chosen_dataset not in [\"adult\", \"bank\"]:\n",
    "#                         return (\n",
    "#                             False,\n",
    "#                             None,\n",
    "#                             f\"Invalid protected. Available list of attributes in the data: {available_columns}.\",\n",
    "#                         )\n",
    "#                     else:\n",
    "#                         k += \" \" + prot + \",\"\n",
    "\n",
    "#             if k != \"\":\n",
    "#                 return (\n",
    "#                     True,\n",
    "#                     user_input.split(),\n",
    "#                     f\"Invalid protected attribute(s) {k} will be replaced with default known protected.\",\n",
    "#                 )\n",
    "\n",
    "#             return True, user_input.split(), \"\"\n",
    "\n",
    "#     elif arg_name == \"nprotgs\":\n",
    "#         # If we're validating from button selection, we already handled the count check\n",
    "#         if \"nprotgs_temp\" in session and len(session[\"nprotgs_temp\"]) > 0:\n",
    "#             return True, user_input.split(), \"\"\n",
    "\n",
    "#         # Otherwise, default validation\n",
    "#         existing_prots = user_args.get(\"prots\", [])\n",
    "#         splitted = user_input.split()\n",
    "#         if len(splitted) != len(existing_prots):\n",
    "#             return (\n",
    "#                 False,\n",
    "#                 None,\n",
    "#                 f\"Got {len(splitted)} non-protected vals for {len(existing_prots)} protected columns. Please match count.\",\n",
    "#             )\n",
    "\n",
    "#     elif arg_name == \"deploy\":\n",
    "#         val = user_input.lower()\n",
    "#         if val not in [\"onnx\", \"pickle\"]:\n",
    "#             return False, None, \"Deployment must be 'onnx' or 'pickle'.\"\n",
    "#         return True, val, \"\"\n",
    "\n",
    "#     elif arg_name == \"moo_vis\":\n",
    "#         if user_input.lower() in [\"true\", \"yes\", \"y\"]:\n",
    "#             return True, True, \"\"\n",
    "#         else:\n",
    "#             return True, False, \"\"\n",
    "\n",
    "#     else:\n",
    "#         return True, user_input, \"\"  # Default case\n",
    "\n",
    "\n",
    "# # Modified get_prompt_for_arg to provide dataset features\n",
    "# def get_prompt_for_arg(arg_name, user_args):\n",
    "#     \"\"\"\n",
    "#     Return a question/prompt for the user and available button options if applicable\n",
    "#     based on arg_name\n",
    "#     \"\"\"\n",
    "#     prompts = {\n",
    "#         \"dataset\": (\n",
    "#             \"Please select a dataset:\",\n",
    "#             [\n",
    "#                 {\"value\": \"adult\", \"text\": \"Adult Dataset\"},\n",
    "#                 {\"value\": \"bank\", \"text\": \"Bank Dataset\"},\n",
    "#                 {\"value\": \"credit\", \"text\": \"Credit Dataset\"},\n",
    "#                 {\"value\": \"kdd\", \"text\": \"KDD Dataset\"},\n",
    "#             ],\n",
    "#         ),\n",
    "#         \"target\": (\n",
    "#             \"Enter the label (target) column in your dataset (e.g. 'income'):\",\n",
    "#             None,\n",
    "#         ),\n",
    "#         \"pos_Class\": (\n",
    "#             \"Enter the positive class label if known (else press Enter to skip):\",\n",
    "#             None,\n",
    "#         ),\n",
    "#         \"n_learners\": (\n",
    "#             \"How many learners / iterations?\",\n",
    "#             [\n",
    "#                 {\"value\": \"50\", \"text\": \"50 Learners\"},\n",
    "#                 {\"value\": \"100\", \"text\": \"100 Learners\"},\n",
    "#                 {\"value\": \"200\", \"text\": \"200 Learners\"},\n",
    "#             ],\n",
    "#         ),\n",
    "#         \"constraint\": (\n",
    "#             \"Please select a fairness constraint:\",\n",
    "#             [\n",
    "#                 {\"value\": \"DP\", \"text\": \"Demographic Parity (DP)\"},\n",
    "#                 {\"value\": \"EP\", \"text\": \"Equal Precision (EP)\"},\n",
    "#                 {\"value\": \"EO\", \"text\": \"Equal Opportunity (EO)\"},\n",
    "#                 {\"value\": \"TPR\", \"text\": \"True Positive Rate (TPR)\"},\n",
    "#                 {\"value\": \"FPR\", \"text\": \"False Positive Rate (FPR)\"},\n",
    "#             ],\n",
    "#         ),\n",
    "#         \"nprotgs\": (\n",
    "#             \"Enter corresponding non-protected spec, e.g. 'White Male 30_60' matching the above columns:\",\n",
    "#             None,\n",
    "#         ),\n",
    "#         \"deploy\": (\n",
    "#             \"Would you like to deploy as 'onnx' or 'pickle'?\",\n",
    "#             [\n",
    "#                 {\"value\": \"onnx\", \"text\": \"ONNX Format\"},\n",
    "#                 {\"value\": \"pickle\", \"text\": \"Pickle Format\"},\n",
    "#             ],\n",
    "#         ),\n",
    "#         \"moo_vis\": (\n",
    "#             \"Do you want to enable multi-objective visualization?\",\n",
    "#             [\n",
    "#                 {\"value\": \"True\", \"text\": \"Yes, enable visualization\"},\n",
    "#                 {\"value\": \"False\", \"text\": \"No, disable visualization\"},\n",
    "#             ],\n",
    "#         ),\n",
    "#         \"data_visualization\": (\n",
    "#             \"Would you like to visualize the distribution of your data?\",\n",
    "#             [\n",
    "#                 {\"value\": \"visualize_yes\", \"text\": \"Yes, show visualization\"},\n",
    "#                 {\"value\": \"visualize_no\", \"text\": \"No, continue without visualization\"},\n",
    "#             ],\n",
    "#         ),\n",
    "#     }\n",
    "\n",
    "#     if arg_name == \"classifier\":\n",
    "#         return (\n",
    "#             \"Please select a model type:\",\n",
    "#             [\n",
    "#                 {\"value\": \"MMM_Fair\", \"text\": \"MMM_Fair (AdaBoost)\"},\n",
    "#                 {\"value\": \"MMM_Fair_GBT\", \"text\": \"MMM_Fair_GBT (Gradient Boosting)\"},\n",
    "#             ],\n",
    "#         )\n",
    "\n",
    "#     if arg_name == \"base_learner\":\n",
    "#         return (\n",
    "#             \"Select a base learner:\",\n",
    "#             [\n",
    "#                 {\"value\": \"tree\", \"text\": \"Decision Tree\"},\n",
    "#                 {\"value\": \"lr\", \"text\": \"Linear Regression\"},\n",
    "#                 {\"value\": \"logistic\", \"text\": \"Logistic Regression\"},\n",
    "#                 {\"value\": \"extratree\", \"text\": \"Extra Tree\"},\n",
    "#             ],\n",
    "#         )\n",
    "\n",
    "#     if arg_name == \"prots\":\n",
    "#         # Get dataset name\n",
    "#         dataset_name = user_args.get(\"dataset\", \"\").lower()\n",
    "#         available_features = []\n",
    "#         recommended_features = []\n",
    "\n",
    "#         if \"data\" in session:\n",
    "#             # Get actual columns from the dataset\n",
    "#             available_features = session[\"data\"].data.columns.tolist()\n",
    "#             DATASET_FEATURES[dataset_name] = available_features\n",
    "#         elif dataset_name in DATASET_FEATURES:\n",
    "#             # Use cached features if available\n",
    "#             available_features = DATASET_FEATURES[dataset_name]\n",
    "\n",
    "#         # Get recommended features for this dataset\n",
    "#         if dataset_name in DATASET_RECOMMENDATIONS:\n",
    "#             recommended_features = DATASET_RECOMMENDATIONS[dataset_name]\n",
    "\n",
    "#         # Build a special message with recommendations\n",
    "#         message = \"Please select protected attributes:\"\n",
    "#         if recommended_features:\n",
    "#             message += f\"\\n\\nRecommended for {dataset_name.capitalize()}: \" + \", \".join(\n",
    "#                 recommended_features\n",
    "#             )\n",
    "\n",
    "#         # Return a special format with available features\n",
    "#         return (\n",
    "#             message,\n",
    "#             {\n",
    "#                 \"type\": \"features_selector\",\n",
    "#                 \"available\": available_features,\n",
    "#                 \"recommended\": recommended_features,\n",
    "#             },\n",
    "#         )\n",
    "\n",
    "#     elif arg_name == \"nprotgs\":\n",
    "#         dataset_name = user_args.get(\"dataset\", \"\").lower()\n",
    "#         protected_attrs = user_args.get(\"prots\", [])\n",
    "\n",
    "#         if dataset_name in NONPROTECTED_SUGGESTIONS and protected_attrs:\n",
    "#             # If we've started selecting, show for the next attribute\n",
    "#             if \"nprotgs_temp\" in session and session[\"nprotgs_temp\"]:\n",
    "#                 next_idx = len(session[\"nprotgs_temp\"])\n",
    "#                 if next_idx < len(protected_attrs):\n",
    "#                     next_prot = protected_attrs[next_idx]\n",
    "#                     if next_prot in NONPROTECTED_SUGGESTIONS[dataset_name]:\n",
    "#                         options = NONPROTECTED_SUGGESTIONS[dataset_name][next_prot]\n",
    "#                         return (\n",
    "#                             f\"Select non-protected value for {next_prot}:\",\n",
    "#                             [\n",
    "#                                 {\n",
    "#                                     \"value\": f\"nprotg_{next_prot}_{option['value']}\",\n",
    "#                                     \"text\": option[\"text\"],\n",
    "#                                 }\n",
    "#                                 for option in options\n",
    "#                             ],\n",
    "#                         )\n",
    "#             # If just starting, show for the first attribute\n",
    "#             elif protected_attrs:\n",
    "#                 first_prot = protected_attrs[0]\n",
    "#                 if first_prot in NONPROTECTED_SUGGESTIONS[dataset_name]:\n",
    "#                     options = NONPROTECTED_SUGGESTIONS[dataset_name][first_prot]\n",
    "#                     return (\n",
    "#                         f\"Select non-protected value for {first_prot}:\",\n",
    "#                         [\n",
    "#                             {\n",
    "#                                 \"value\": f\"nprotg_{first_prot}_{option['value']}\",\n",
    "#                                 \"text\": option[\"text\"],\n",
    "#                             }\n",
    "#                             for option in options\n",
    "#                         ],\n",
    "#                     )\n",
    "\n",
    "#         return (\n",
    "#             \"Enter corresponding non-protected spec, e.g. 'White Male' matching the above columns:\",\n",
    "#             None,\n",
    "#         )\n",
    "#     elif arg_name == \"target\":\n",
    "#         dataset_name = user_args.get(\"dataset\", \"\").lower()\n",
    "#         if dataset_name in TARGET_SUGGESTIONS:\n",
    "#             return (\n",
    "#                 \"Enter the label (target) column in your dataset:\",\n",
    "#                 TARGET_SUGGESTIONS[dataset_name],\n",
    "#             )\n",
    "#         else:\n",
    "#             return (\n",
    "#                 \"Enter the label (target) column in your dataset (e.g. 'income'):\",\n",
    "#                 None,\n",
    "#             )\n",
    "\n",
    "#     elif arg_name == \"pos_Class\":\n",
    "#         dataset_name = user_args.get(\"dataset\", \"\").lower()\n",
    "#         target_name = user_args.get(\"target\", \"\").lower()\n",
    "\n",
    "#         # Check if we have suggestions for this dataset and target\n",
    "#         if (\n",
    "#             dataset_name in POS_CLASS_SUGGESTIONS\n",
    "#             and target_name in POS_CLASS_SUGGESTIONS[dataset_name]\n",
    "#         ):\n",
    "#             # Add a skip option and then the suggestions\n",
    "#             options = [{\"value\": \"\", \"text\": \"Skip (use default)\"}]\n",
    "#             options.extend(POS_CLASS_SUGGESTIONS[dataset_name][target_name])\n",
    "\n",
    "#             return (\"Select the positive class label:\", options)\n",
    "#         else:\n",
    "#             return (\n",
    "#                 \"Enter the positive class label if known (else press Enter to skip):\",\n",
    "#                 None,\n",
    "#             )\n",
    "\n",
    "#     return prompts.get(arg_name, (f\"Enter {arg_name}:\", None))\n",
    "\n",
    "\n",
    "# # Add a new route to handle feature selection\n",
    "# @app.route(\"/select_feature\", methods=[\"POST\"])\n",
    "# def select_feature():\n",
    "#     data = request.json\n",
    "#     feature = data.get(\"feature\", \"\")\n",
    "\n",
    "#     # Get current selected features\n",
    "#     user_args = session.get(\"user_args\", {})\n",
    "#     current_prots = user_args.get(\"prots\", [])\n",
    "\n",
    "#     # Add the feature if not already selected\n",
    "#     if feature and feature not in current_prots:\n",
    "#         current_prots.append(feature)\n",
    "#         user_args[\"prots\"] = current_prots\n",
    "#         session[\"user_args\"] = user_args\n",
    "\n",
    "#     return jsonify({\"success\": True, \"selected_features\": current_prots})\n",
    "\n",
    "\n",
    "# @app.route(\"/visualize_data\", methods=[\"POST\"])\n",
    "# def visualize_data():\n",
    "#     \"\"\"Generate and return a visualization of the dataset distribution\"\"\"\n",
    "#     # Get data from session\n",
    "#     if \"data\" not in session:\n",
    "#         return jsonify(\n",
    "#             {\n",
    "#                 \"success\": False,\n",
    "#                 \"error\": \"No dataset loaded. Please load a dataset first.\",\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     data_obj = session.get(\"data\")\n",
    "#     user_args = session.get(\"user_args\", {})\n",
    "\n",
    "#     # Get target and protected attributes\n",
    "#     target = user_args.get(\"target\")\n",
    "#     protected_attrs = user_args.get(\"prots\", [])\n",
    "\n",
    "#     if not target or not protected_attrs:\n",
    "#         return jsonify(\n",
    "#             {\"success\": False, \"error\": \"Target or protected attributes not defined.\"}\n",
    "#         )\n",
    "\n",
    "#     try:\n",
    "#         # Generate the plot using your existing function\n",
    "#         df = data_obj.data.copy()  # Get the dataframe from your data object\n",
    "#         df[target] = data_obj.labels[\"label\"].values.copy()\n",
    "#         # Generate a unique filename\n",
    "#         # unique_id = str(uuid.uuid4())[:8]\n",
    "#         # pie_plot_filename = f\"data_dist_{unique_id}.html\"\n",
    "#         # pie_plot_path = os.path.join(PLOT_DIR, pie_plot_filename)\n",
    "\n",
    "#         # Generate the nested pie chart\n",
    "#         data_plot_html = generate_nested_pie_chart(df, [target] + protected_attrs)\n",
    "\n",
    "        \n",
    "#         style_dict = {  \n",
    "#             \"width\": \"100%\",\n",
    "#             \"height\": \"500px\",\n",
    "#             \"overflow\": \"hidden\",\n",
    "#             \"border\": \"None\",     \n",
    "#         }\n",
    "\n",
    "#         data_plot_dict = {\n",
    "#             \"srcdoc\": data_plot_html,\n",
    "#             \"style\": style_dict,\n",
    "#             \"id\": str(uuid.uuid4())[:4]\n",
    "#         }\n",
    "\n",
    "#         # print(plot_html)\n",
    "#         # # Save the plot\n",
    "#         # with open(pie_plot_path, \"w\") as f:\n",
    "#         #     f.write(plot_html)\n",
    "#         #     f.flush()\n",
    "#         #     os.fsync(f.fileno())\n",
    "#         #     # Get the next prompt to display after visualization\n",
    "\n",
    "#         # Mark visualization as completed in user_args\n",
    "#         user_args[\"data_visualization\"] = \"completed\"\n",
    "#         session[\"user_args\"] = user_args\n",
    "\n",
    "#         # Get the next prompt to display after visualization\n",
    "#         chat_history = session.get(\"chat_history\", [])\n",
    "#         chat_history.append(\n",
    "#             {\n",
    "#                 \"sender\": \"bot\",\n",
    "#                 \"text\": \"Here's the distribution of your data based on target and protected attributes.\",\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#         # Determine the next argument to prompt for\n",
    "#         new_missing = get_missing_args(user_args)\n",
    "\n",
    "#         if new_missing:\n",
    "#             next_arg = new_missing[0]\n",
    "#             prompt, options = get_prompt_for_arg(next_arg, user_args)\n",
    "\n",
    "#             next_message = {\"sender\": \"bot\", \"text\": prompt}\n",
    "#             if options:\n",
    "#                 next_message[\"options\"] = options\n",
    "\n",
    "#             chat_history.append(next_message)\n",
    "#         else:\n",
    "#             # If no more arguments needed, show the run options\n",
    "#             chat_history.append(\n",
    "#                 {\n",
    "#                     \"sender\": \"bot\",\n",
    "#                     \"text\": \"All arguments captured. What would you like to do?\",\n",
    "#                     \"options\": [\n",
    "#                         {\"value\": \"run\", \"text\": \"Run Training\"},\n",
    "#                         {\"value\": \"reset\", \"text\": \"Start Over\"},\n",
    "#                     ],\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#         session[\"chat_history\"] = chat_history\n",
    "\n",
    "#         return jsonify(\n",
    "#             {\n",
    "#                 \"success\": True,\n",
    "#                 \"plots\": [data_plot_dict],  # Send HTML directly instead of file path\n",
    "#                 \"chat_history\": chat_history[\n",
    "#                     -2:\n",
    "#                 ],  # Return both visualization message and next prompt\n",
    "#             }\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in visualization: {str(e)}\")\n",
    "#         return jsonify(\n",
    "#             {\"success\": False, \"error\": f\"Error generating visualization: {str(e)}\"}\n",
    "#         )\n",
    "\n",
    "\n",
    "# @app.route(\"/finish_features\", methods=[\"POST\"])\n",
    "# def finish_features():\n",
    "#     user_args = session.get(\"user_args\", {})\n",
    "#     selected_features = user_args.get(\"prots\", [])\n",
    "\n",
    "#     if not selected_features:\n",
    "#         return jsonify(\n",
    "#             {\n",
    "#                 \"success\": False,\n",
    "#                 \"error\": \"Please select at least one protected attribute.\",\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     # Convert to the format expected by the existing code\n",
    "#     chat_history = session.get(\"chat_history\", [])\n",
    "\n",
    "#     # Add message showing selected features\n",
    "#     chat_history.append(\n",
    "#         {\n",
    "#             \"sender\": \"bot\",\n",
    "#             \"text\": f\"Selected protected attributes: {', '.join(selected_features)}\",\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     # Check if we have both target and protected attributes\n",
    "#     if \"target\" in user_args and user_args[\"target\"]:\n",
    "#         # Add suggestion to visualize data\n",
    "#         chat_history.append(\n",
    "#             {\n",
    "#                 \"sender\": \"bot\",\n",
    "#                 \"text\": \"Would you like to visualize the distribution of your data?\",\n",
    "#                 \"options\": [\n",
    "#                     {\"value\": \"visualize_yes\", \"text\": \"Yes, show visualization\"},\n",
    "#                     {\n",
    "#                         \"value\": \"visualize_no\",\n",
    "#                         \"text\": \"No, continue without visualization\",\n",
    "#                     },\n",
    "#                 ],\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#         session[\"chat_history\"] = chat_history\n",
    "#         return jsonify({\"success\": True, \"chat_history\": chat_history[-2:]})\n",
    "\n",
    "#     # If no target yet, continue with original flow\n",
    "#     # Continue to next parameter\n",
    "#     new_missing = get_missing_args(user_args)\n",
    "#     if new_missing:\n",
    "#         next_arg = new_missing[0]\n",
    "#         prompt, options = get_prompt_for_arg(next_arg, user_args)\n",
    "\n",
    "#         message = {\"sender\": \"bot\", \"text\": prompt}\n",
    "#         if options:\n",
    "#             message[\"options\"] = options\n",
    "\n",
    "#         chat_history.append(message)\n",
    "#     else:\n",
    "#         chat_history.append(\n",
    "#             {\n",
    "#                 \"sender\": \"bot\",\n",
    "#                 \"text\": \"All arguments captured. What would you like to do?\",\n",
    "#                 \"options\": [\n",
    "#                     {\"value\": \"run\", \"text\": \"Run Training\"},\n",
    "#                     {\"value\": \"reset\", \"text\": \"Start Over\"},\n",
    "#                 ],\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     session[\"chat_history\"] = chat_history\n",
    "#     return jsonify({\"success\": True, \"chat_history\": chat_history[-2:]})\n",
    "\n",
    "\n",
    "# @app.route(\"/reset_chat\")\n",
    "# def reset_chat():\n",
    "#     session.pop(\"chat_history\", None)\n",
    "#     session.pop(\"user_args\", None)\n",
    "#     session.pop(\"data\", None)\n",
    "#     for fname in os.listdir(PLOT_DIR):\n",
    "#         if (\n",
    "#             fname.startswith(\"table_\")\n",
    "#             or fname.startswith(\"fair_\")\n",
    "#             or fname.startswith(\"all_\")\n",
    "#         ):\n",
    "#             try:\n",
    "#                 os.remove(os.path.join(PLOT_DIR, fname))\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "#     return \"Chat reset done.\"\n",
    "\n",
    "\n",
    "# def run_mmm_fair_app(user_args):\n",
    "#     args = argparse.Namespace(**user_args)\n",
    "#     mmm_classifier, X_test, y_test, saIndex_test, sensitives = train(args)\n",
    "#     session[\"mmm_classifier\"] = mmm_classifier\n",
    "#     session[\"xtest\"] = X_test\n",
    "#     session[\"ytest\"] = y_test\n",
    "#     session[\"saIndex_test\"] = saIndex_test\n",
    "#     session[\"sensitives\"] = sensitives\n",
    "#     PF = np.array([mmm_classifier.ob[i] for i in range(len(mmm_classifier.ob))])\n",
    "#     thetas = np.arange(len(mmm_classifier.ob))\n",
    "#     title = f\"3D Scatter Plot. Showing various trade-off points between Accuracy, Balanced Accuracy, and Maximum violation of {mmm_classifier.constraint} fairness among protected attributes.\"\n",
    "\n",
    "#     vis_all = plot_spider(\n",
    "#         objectives=PF,\n",
    "#         theta=thetas,\n",
    "#         criteria=\"Multi\",\n",
    "#         axis_names=[\"Acc.\", \"Balanc. Acc\", \"MMM-fair\"],\n",
    "#         title=title,\n",
    "#         html=True,\n",
    "#     )\n",
    "#     # vis_all = plot3d(\n",
    "#     #     x=PF[:, 0],\n",
    "#     #     y=PF[:, 1],\n",
    "#     #     z=PF[:, 2],\n",
    "#     #     theta=thetas,\n",
    "#     #     criteria=\"Multi\",\n",
    "#     #     axis_names=[\"Acc.\", \"Balanc. Acc\", \"MMM-fair\"],\n",
    "#     #     title=title,\n",
    "#     #     html=True,\n",
    "#     # )\n",
    "#     PF = np.array(\n",
    "#         [mmm_classifier.fairobs[i] for i in range(len(mmm_classifier.fairobs))]\n",
    "#     )\n",
    "#     title = \"3D Scatter Plot. Showing various trade-off points between maximum violation of Demopgraphic Parity, Equal Opportunity, and Equalized odds fairness for the given set of protected attributes.\"\n",
    "#     vis_fair = plot_spider(\n",
    "#         objectives=PF,\n",
    "#         theta=thetas,\n",
    "#         criteria=\"Multi-definitions\",\n",
    "#         axis_names=[\"DP\", \"EqOpp\", \"EqOdd\", \"TPR\", \"FPR\"],\n",
    "#         title=title,\n",
    "#         html=True,\n",
    "#     )\n",
    "\n",
    "#     # vis_fair = plot3d(\n",
    "#     #     x=PF[:, 0],\n",
    "#     #     y=PF[:, 1],\n",
    "#     #     z=PF[:, 2],\n",
    "#     #     theta=thetas,\n",
    "#     #     criteria=\"Multi-definitions\",\n",
    "#     #     axis_names=[\"DP\", \"EqOpp\", \"EqOdd\"],\n",
    "#     #     title=title,\n",
    "#     #     html=True,\n",
    "#     # )\n",
    "\n",
    "#     y_pred = mmm_classifier.predict(X_test)\n",
    "#     report_table = generate_reports(\n",
    "#         \"html\", sensitives, mmm_classifier, saIndex_test, y_pred, y_test, html=True\n",
    "#     )\n",
    "#     # plot_all = \"all_.html\"\n",
    "#     # plot_fair = \"fair_.html\"\n",
    "#     # plot_table = \"table_.html\"\n",
    "\n",
    "#     # plot_all_path = os.path.join(PLOT_DIR, plot_all)\n",
    "#     # plot_fair_path = os.path.join(PLOT_DIR, plot_fair)\n",
    "#     # plot_table_path = os.path.join(PLOT_DIR, plot_table)\n",
    "\n",
    "#     # # Save the Plotly-generated HTML directly to files\n",
    "#     # with open(plot_all_path, \"w\") as f:\n",
    "#     #     f.write(vis_all)\n",
    "#     # with open(plot_fair_path, \"w\") as f:\n",
    "#     #     f.write(vis_fair)\n",
    "#     # with open(plot_table_path, \"w\") as f:\n",
    "#     #     f.write(report_table)\n",
    "\n",
    "#     style_dict = {  \n",
    "#         \"width\": \"100%\",\n",
    "#         \"height\": \"500px\",\n",
    "#         \"overflow\": \"hidden\",\n",
    "#         \"border\": \"None\",     \n",
    "#     }\n",
    "\n",
    "#     vis_all_plot_dict = {\n",
    "#         \"srcdoc\": vis_all,\n",
    "#         \"style\": style_dict,\n",
    "#         \"id\": str(uuid.uuid4())[:4]\n",
    "#     }\n",
    "#     vis_fair_plot_dict = {\n",
    "#         \"srcdoc\": vis_fair,\n",
    "#         \"style\": style_dict,\n",
    "#         \"id\": str(uuid.uuid4())[:4]\n",
    "#     }    \n",
    "#     table_id = str(uuid.uuid4())[:4]\n",
    "#     report_table_plot_dict = {\n",
    "#         \"srcdoc\": report_table,\n",
    "#         \"style\": style_dict,\n",
    "#         \"id\": table_id\n",
    "#     }\n",
    "#     session[\"plots\"] = [vis_all_plot_dict, vis_fair_plot_dict, report_table_plot_dict]\n",
    "#     session[\"table_id\"] = table_id\n",
    "#     session[\"html_divs\"] = [THETA_DIV]\n",
    "\n",
    "#     return [vis_all_plot_dict, vis_fair_plot_dict, report_table_plot_dict], [THETA_DIV]\n",
    "\n",
    "\n",
    "# @app.route(\"/static/<path:filename>\")\n",
    "# def serve_static_files(filename):\n",
    "#     return send_from_directory(\"static\", filename)\n",
    "\n",
    "\n",
    "# @app.route(\"/static/<path:filename>\")\n",
    "# def serve_plot(filename):\n",
    "#     plot_dir = os.path.abspath(\"static/plots\")  # Ensure absolute path\n",
    "#     return send_from_directory(plot_dir, filename)\n",
    "\n",
    "\n",
    "# @app.route(\"/save_model\", methods=[\"POST\"])\n",
    "# def save_model():\n",
    "#     data = request.json\n",
    "#     save_path = data.get(\"save_path\", \"\").strip()\n",
    "\n",
    "#     # Retrieve trained classifier\n",
    "#     mmm_classifier = session.get(\"mmm_classifier\")\n",
    "#     clf = session.get(\"classifier\")\n",
    "#     xdata = session.get(\"xtest\")\n",
    "#     user_args = session.get(\"user_args\", {})\n",
    "\n",
    "#     if not mmm_classifier:\n",
    "#         return jsonify(\n",
    "#             {\"success\": False, \"error\": \"No trained model found! Run training first.\"}\n",
    "#         )\n",
    "\n",
    "#     # Validate save path\n",
    "#     if not save_path or not os.path.isdir(save_path):\n",
    "#         return jsonify(\n",
    "#             {\n",
    "#                 \"success\": False,\n",
    "#                 \"error\": \"Invalid directory. Please select a valid folder.\",\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     # Call deploy() to save the model in the user-specified directory\n",
    "#     try:\n",
    "#         user_args[\"save_path\"] = save_path  # Update args with the user-selected path\n",
    "#         convert_to_onnx(mmm_classifier, save_path, xdata, clf)\n",
    "#         return jsonify(\n",
    "#             {\n",
    "#                 \"success\": True,\n",
    "#                 \"message\": f\"Model saved in {save_path}\",\n",
    "#                 \"chat_history\": [\n",
    "#                     {\n",
    "#                         \"sender\": \"bot\",\n",
    "#                         \"text\": f\"Model successfully saved in {save_path}. What would you like to do next?\",\n",
    "#                         \"options\": [\n",
    "#                             {\"value\": \"reset\", \"text\": \"Start Over\"},\n",
    "#                             {\"value\": \"exit\", \"text\": \"Exit\"},\n",
    "#                         ],\n",
    "#                     }\n",
    "#                 ],\n",
    "#             }\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         return jsonify({\"success\": False, \"error\": str(e)})\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     app.run(debug=True)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
